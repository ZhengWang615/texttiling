The included file, textTiling.py, represents a very rudimentary attempt at implementing the TextTiling algorithm as described in the Hearst paper. To run the algorithm, please run 'python textTiling.py' in the terminal. As a result, the input text will be displayed, followed by the segmented text into subtopics according to the TextTiling algorithm.

The TextTiling algorithm comprises three main steps: tokenization of text, lexically scoring the resulting token-sequences, and determining the subtopic boundaries based on these lexical scores. Arguably, a precursor step involves retrieving/sanitizing input text to feed into tokenization. Here I describe my progress and future goals for each step.

With respect to the precursor step, the input text at the moment is simply a toy example that was designed to replicate Figure 3 of the Hearst paper. As such, every word is represented as a single letter (A through I, as well as a stop word X), and both w and k were chosen specifically to follow the Figure 3 implications directly and segment the text accurately. I created two stub functions (getText and sanitizeText) for respectively retrieving and sanitizing input text (as a group, we will select a training corpus, and then these functions can be refined).

With respect to the tokenization step, I used nltk's word_tokenize function to tokenize the input text and recognize paragraphs by splitting on double newlines (the convention used, for instance, for this README). Future revisions might need a more robust way to recognize paragraphs. Other future revisions include the morphological root normalization referenced in the paper as well as the incorporation of stop words, both of which are available as nltk functionality but not included in this initial tokenization due to time restrictions. The final result of tokenization is a record of token-sequences with counts of their constituent tokens as well as a record of paragraph breaks, used later for displaying the results of the TextTiling algorithm as applied to the input text.

With respect to the lexical scoring step, the paper discusses three distinct possibilities for assigning lexical scores to each gap between token-sequences. In this version, I implemented the block comparison method that combines a certain number (k) of token-sequences into blocks to be compared for lexical similarity. To handle boundary/edge token-sequences, I tightened the window for these gaps to the maximum allowable size. In future revisions, this method of edge-case handling may be changed/improved, and we will also implement vocabulary introduction and lexical chaining with opportunities for comparison of relative performance.

With respect to the boundary identification step, I implemented the key components required but have ample room for refinement. The implemented features include determination of the depth score cutoff (depending on whether the liberal or conservative criterion is used), determination of the depth scores for all token-sequence gaps based on their lexical scores, and fitting of the raw subtopic boundaries (which may follow any token-sequence) into boundaries that exactly coincide with paragraph breaks (determined from the tokenization step). Future revisions include the incorporation of localized lexical distribution information to handle depth score distributions like that in Figure 4(c) more gracefully, the addition of the smoothing method described in Section 5.4 of the paper, and the prevention of assigning very close adjacent boundaries (i.e., within three token-sequences to another already assigned boundary) as described in Section 5.3. The latter revisions, although not included in this initial version, are straightforward to implement. The first revision (incorporating localized lexical distribution information) appears more ambiguous and, perhaps in part because of that, more daunting - advice on this aspect would be warmly appreciated!

Finally, the results of the algorithm are displayed in a somewhat human-friendly format, though this may be subject to change as the input text grows drastically in size. More broadly speaking, we would like to focus in the future on tuning of the parameters k (block size), w (number of words per token-sequence), and s (for smoothing), perhaps via cross-validation on a large corpus. We will also implement various evaluation mechanisms based on another Hearst paper (A Critique and Improvement of an Evaluation Metric for Text Segmentation, 2002), perhaps beginning first with comparison to a randomly segmenting baseline. We have made progress with this initial implementation phase and have many more opportunities for progress with future implementations!